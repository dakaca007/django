# crawler.py
import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime, timedelta
import concurrent.futures
import time
import random

# é…ç½®
START_ID = 861573
END_ID = 960020
INITIAL_DATE = datetime(2017, 5, 11)
MAX_DATE_SHIFT = 7
BATCH_SIZE = 50
MAX_WORKERS = 5


# å¦‚æœ songs_meta.json ä¸å­˜åœ¨ï¼Œå…ˆåˆ›å»ºä¸€ä¸ªç©ºçš„ JSON æ•°ç»„
if not os.path.exists("songs_meta.json"):
    with open("songs_meta.json", "w", encoding="utf-8") as f:
        f.write("[]")

# å¦‚æœ failed_songs.txt ä¸å­˜åœ¨ï¼Œå°±åˆ›å»ºä¸€ä¸ªç©ºæ–‡ä»¶
if not os.path.exists("failed_songs.txt"):
    open("failed_songs.txt", "a", encoding="utf-8").close()

SONGS_JSON = "songs_meta.json"
FAILED_FILE = "failed_songs.txt"
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

session = requests.Session()
session.headers.update(HEADERS)

def load_songs():
    if os.path.exists(SONGS_JSON):
        with open(SONGS_JSON, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_songs(songs):
    with open(SONGS_JSON, "w", encoding="utf-8") as f:
        json.dump(songs, f, ensure_ascii=False, indent=2)

def log_failed(song_id):
    with open(FAILED_FILE, "a", encoding="utf-8") as f:
        f.write(f"{song_id}\n")

def find_mp3_url(song_id, base_date):
    base = "https://music.jsbaidu.com/upload/128"
    dates = [base_date + timedelta(days=i) for i in range(MAX_DATE_SHIFT)]
    for d in dates:
        url = f"{base}/{d:%Y/%m/%d}/{song_id}.mp3"
        try:
            r = session.head(url, timeout=3)
            if r.status_code == 200:
                print(f"ğŸ¯ MP3 URL: {url}")
                return url
        except:
            pass
    return None

def get_song_info(song_id):
    url = f"https://www.9ku.com/play/{song_id}.htm"
    res = session.get(url, timeout=10)
    if res.status_code != 200:
        return None

    soup = BeautifulSoup(res.text, "html.parser")
    title = soup.find("h1").text.strip() if soup.find("h1") else f"æœªçŸ¥æ­Œæ›²_{song_id}"
    artist = soup.find("h2").text.strip() if soup.find("h2") else "æœªçŸ¥æ­Œæ‰‹"

    info = {
        "song_id": song_id,
        "title": title,
        "artist": artist,
        "play_url": f"/play/{song_id}"
    }

    # ä¸“è¾‘ & å‘è¡Œæ—¶é—´
    div = soup.find("div", class_="songText")
    if div:
        for p in div.find_all("p", class_="p1"):
            t = p.get_text()
            if "å‘è¡Œæ—¶é—´ï¼š" in t:
                info["release_date"] = t.split("å‘è¡Œæ—¶é—´ï¼š")[1].split("&")[0].strip()
            if "æ‰€å±ä¸“è¾‘ï¼š" in t:
                info["album"] = p.find("a").text.strip() if p.find("a") else None

    # æ­Œè¯ URL
    lyric = soup.find("textarea", id="lrc_content")
    if lyric and lyric.text.strip():
        info["has_lyric"] = True
        info["lyric_url"] = f"/lyric/{song_id}"
    else:
        info["has_lyric"] = False

    # MP3 URLï¼ˆåŸºäºåˆå§‹æ—¥æœŸå°è¯•ï¼‰
    mp3_url = find_mp3_url(song_id, INITIAL_DATE)
    if mp3_url:
        info["mp3_url"] = mp3_url
    else:
        info["mp3_url"] = None

    return info

def crawl_songs():
    songs = load_songs()
    existing = {s["song_id"] for s in songs}

    for song_id in range(START_ID, END_ID + 1):
        if song_id in existing:
            continue

        info = get_song_info(song_id)
        if info:
            songs.append(info)
            print(f"âœ… çˆ¬å–æˆåŠŸ: {info['title']} (ID:{song_id})")
        else:
            log_failed(song_id)
            print(f"âŒ çˆ¬å–å¤±è´¥: ID {song_id}")

        if len(songs) % BATCH_SIZE == 0:
            save_songs(songs)
            # éšæœºçŸ­æš‚ä¼‘çœ ï¼Œé˜²æ­¢è¢«å°
            time.sleep(random.uniform(0.5, 1.5))

    save_songs(songs)
    print("ğŸ‰ çˆ¬å–å®Œæˆï¼Œå·²ä¿å­˜åˆ°", SONGS_JSON)

if __name__ == "__main__":
    crawl_songs()
