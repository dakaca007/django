# crawler.py
import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime, timedelta
import concurrent.futures
import time
import random
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# é…ç½®
START_ID = 861573
END_ID = 861600  # æµ‹è¯•èŒƒå›´ï¼ŒæˆåŠŸåæ”¹ä¸º960020
INITIAL_DATE = datetime(2017, 5, 11)
MAX_DATE_SHIFT = 14
BATCH_SIZE = 10  # å°æ‰¹é‡æµ‹è¯•
MAX_WORKERS = 3  # ä½å¹¶å‘æµ‹è¯•

SONGS_JSON = "songs_meta.json"
FAILED_FILE = "failed_songs.txt"
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15"
]

def ensure_directories():
    """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
    for path in [SONGS_JSON, FAILED_FILE]:
        directory = os.path.dirname(path)
        if directory and not os.path.exists(directory):
            os.makedirs(directory)

def load_songs():
    """å®‰å…¨åŠ è½½å·²æœ‰æ­Œæ›²æ•°æ®"""
    try:
        if os.path.exists(SONGS_JSON):
            with open(SONGS_JSON, "r", encoding="utf-8") as f:
                return json.load(f)
        return []
    except (json.JSONDecodeError, IOError) as e:
        logger.error(f"åŠ è½½æ­Œæ›²æ•°æ®å¤±è´¥: {e}")
        return []

def save_songs(songs):
    """ä¿å­˜æ­Œæ›²æ•°æ®åˆ°æ–‡ä»¶"""
    try:
        with open(SONGS_JSON, "w", encoding="utf-8") as f:
            json.dump(songs, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(songs)} é¦–æ­Œæ›²æ•°æ®")
        return True
    except IOError as e:
        logger.error(f"ä¿å­˜æ­Œæ›²æ•°æ®å¤±è´¥: {e}")
        return False

def log_failed(song_id, reason=""):
    """è®°å½•å¤±è´¥çš„æ­Œæ›²ID"""
    try:
        with open(FAILED_FILE, "a", encoding="utf-8") as f:
            f.write(f"{song_id}: {reason}\n")
        logger.warning(f"è®°å½•å¤±è´¥ID: {song_id} - {reason}")
    except IOError as e:
        logger.error(f"è®°å½•å¤±è´¥IDå¤±è´¥: {e}")

def find_mp3_url(song_id, base_date):
    """æŸ¥æ‰¾æœ‰æ•ˆçš„MP3æ–‡ä»¶URL"""
    # ä¿®å¤URLåè®® (æ·»åŠ å†’å·)
    base = "https://music.jsbaidu.com/upload/128"
    
    # åŒå‘æ—¥æœŸæœç´¢ (å‰7å¤©å7å¤©)
    date_range = range(-MAX_DATE_SHIFT//2, MAX_DATE_SHIFT//2 + 1)
    dates = [base_date + timedelta(days=i) for i in date_range]
    
    for d in dates:
        url = f"{base}/{d.strftime('%Y/%m/%d')}/{song_id}.mp3"
        try:
            # ä½¿ç”¨GETè¯·æ±‚+æµå¼ä¼ è¾“
            with requests.get(
                url, 
                timeout=8,
                headers={'User-Agent': random.choice(USER_AGENTS)},
                stream=True
            ) as r:
                # å…³é”®éªŒè¯ç‚¹
                if r.status_code == 200:
                    content_type = r.headers.get('Content-Type', '').lower()
                    content_length = int(r.headers.get('Content-Length', 0))
                    
                    # éªŒè¯éŸ³é¢‘æ–‡ä»¶å’Œåˆç†å¤§å°
                    if ('audio' in content_type or 'mpeg' in content_type) and 1024 < content_length < 10*1024*1024:
                        logger.info(f"âœ… æ‰¾åˆ°æœ‰æ•ˆMP3: {song_id} @ {d.date()}")
                        return url
                    else:
                        logger.debug(f"æ— æ•ˆMP3: {url} (Type: {content_type}, Size: {content_length//1024}KB)")
        except (requests.ConnectionError, requests.Timeout, requests.RequestException) as e:
            logger.debug(f"MP3æ£€æŸ¥å¤±è´¥ {url}: {str(e)[:50]}")
    return None

def get_song_info(song_id):
    """è·å–å•é¦–æ­Œæ›²çš„å…ƒä¿¡æ¯"""
    headers = {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    }
    
    try:
        # ä¿®å¤URLåè®® (æ·»åŠ å†’å·)
        url = f"https://www.9ku.com/play/{song_id}.htm"
        logger.info(f"ğŸ” è¯·æ±‚: {url}")
        
        res = requests.get(url, headers=headers, timeout=15)
        res.raise_for_status()
        
        # å†…å®¹ç±»å‹éªŒè¯
        if 'text/html' not in res.headers.get('Content-Type', ''):
            logger.warning(f"âš ï¸ æ— æ•ˆå†…å®¹ç±»å‹: {res.headers.get('Content-Type')}")
            return None

        soup = BeautifulSoup(res.text, "html.parser")
        
        # æ ‡é¢˜æå– (ä¼˜å…ˆä½¿ç”¨IDé€‰æ‹©å™¨)
        title_element = soup.find("h1", id="songName") or soup.find("h1")
        title = title_element.text.strip() if title_element else f"æœªçŸ¥æ­Œæ›²_{song_id}"
        
        # æ­Œæ‰‹æå–
        artist_element = soup.find("h2", id="artistName") or soup.find("h2")
        artist = artist_element.text.strip() if artist_element else "æœªçŸ¥æ­Œæ‰‹"

        info = {
            "song_id": song_id,
            "title": title,
            "artist": artist,
            "play_url": url,
            "mp3_url": None,
            "has_lyric": False,
            "album": None,
            "release_date": None
        }

        # ä¸“è¾‘ä¿¡æ¯æå–
        song_info_div = soup.find("div", class_="songInfo") or soup.find("div", class_="songText")
        if song_info_div:
            for p in song_info_div.find_all("p"):
                text = p.get_text(strip=True)
                if "å‘è¡Œæ—¶é—´ï¼š" in text:
                    info["release_date"] = text.split("å‘è¡Œæ—¶é—´ï¼š")[1].split("&")[0].strip()
                if "æ‰€å±ä¸“è¾‘ï¼š" in text:
                    a_tag = p.find("a")
                    if a_tag:
                        info["album"] = a_tag.get_text(strip=True)

        # æ­Œè¯æ£€æŸ¥
        if soup.find("textarea", id="lrc_content"):
            info["has_lyric"] = True
            info["lyric_url"] = f"https://www.9ku.com/lyric/{song_id}.htm"

        # MP3æŸ¥æ‰¾
        mp3_url = find_mp3_url(song_id, INITIAL_DATE)
        info["mp3_url"] = mp3_url
        
        logger.info(f"âœ… æˆåŠŸè§£æ: {title} - {artist} (ID:{song_id})")
        return info
        
    except requests.HTTPError as e:
        status = e.response.status_code
        logger.warning(f"ğŸš« HTTPé”™è¯¯ {song_id}: {status}")
        log_failed(song_id, f"HTTP_{status}")
        return None
    except requests.RequestException as e:
        logger.warning(f"ğŸš« è¯·æ±‚å¼‚å¸¸ {song_id}: {str(e)[:50]}")
        log_failed(song_id, f"REQ_EXCEPTION")
        return None
    except Exception as e:
        logger.error(f"ğŸš« è§£æå¼‚å¸¸ {song_id}: {str(e)[:100]}", exc_info=True)
        log_failed(song_id, f"PARSE_ERROR")
        return None

def crawl_songs():
    """ä¸»çˆ¬å–å‡½æ•°"""
    ensure_directories()
    songs = load_songs()
    existing_ids = {s["song_id"] for s in songs}
    
    total_ids = END_ID - START_ID + 1
    new_count = 0
    
    logger.info(f"ğŸ“Š ä»»åŠ¡ç»Ÿè®¡: å…±{total_ids}é¦– | å·²å­˜åœ¨{len(existing_ids)}é¦– | å¾…çˆ¬å–{total_ids - len(existing_ids)}é¦–")
    
    # åˆ›å»ºä»»åŠ¡åˆ—è¡¨ (è·³è¿‡å·²å­˜åœ¨)
    todo_ids = [sid for sid in range(START_ID, END_ID + 1) if sid not in existing_ids]
    random.shuffle(todo_ids)
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(get_song_info, song_id): song_id for song_id in todo_ids}
        
        for i, future in enumerate(concurrent.futures.as_completed(futures)):
            song_id = futures[future]
            try:
                info = future.result()
                if info:
                    songs.append(info)
                    new_count += 1
                    logger.info(f"ğŸµ è¿›åº¦: {i+1}/{len(todo_ids)} | æ–°å¢: {new_count}/{BATCH_SIZE}")
                    
                    # æ‰¹é‡ä¿å­˜
                    if new_count >= BATCH_SIZE:
                        if save_songs(songs):
                            new_count = 0
            except Exception as e:
                logger.error(f"ğŸ”¥ å¤„ç†å¼‚å¸¸ {song_id}: {str(e)[:100]}", exc_info=True)
            
            # åŠ¨æ€å»¶è¿Ÿ (0.3-1.5ç§’)
            time.sleep(random.uniform(0.3, 1.5))
    
    # æœ€ç»ˆä¿å­˜
    save_songs(songs)
    success_count = len([s for s in songs if s.get("mp3_url")])
    logger.info(f"ğŸ‰ çˆ¬å–å®Œæˆ! æ€»è®¡: {len(songs)}é¦– | å«MP3: {success_count}é¦– | å¤±è´¥: {len(todo_ids) - len(songs)}é¦–")

if __name__ == "__main__":
    try:
        crawl_songs()
    except KeyboardInterrupt:
        logger.warning("â›” ç”¨æˆ·ä¸­æ–­ï¼Œå°è¯•ä¿å­˜æ•°æ®...")
        # ç«‹å³ä¿å­˜å½“å‰è¿›åº¦
        import traceback
        traceback.print_exc()
    except Exception as e:
        logger.error(f"ğŸ’¥ ç¨‹åºå´©æºƒ: {str(e)}", exc_info=True)